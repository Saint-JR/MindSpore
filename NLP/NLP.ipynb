{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 实验数据下载"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://zhuanyejianshe.obs.cn-north-4.myhuaweicloud.com/chuangxinshijianke/cmn_zhsim.zip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 导入第三方模块"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import re\n",
    "import sys\n",
    "import random\n",
    "import unicodedata\n",
    "import math\n",
    "\n",
    "from mindspore import Tensor, nn, Model, context\n",
    "from mindspore.train.serialization import load_param_into_net, load_checkpoint\n",
    "from mindspore.train.callback import LossMonitor, CheckpointConfig, ModelCheckpoint, TimeMonitor\n",
    "from mindspore import dataset as ds\n",
    "from mindspore.mindrecord import FileWriter\n",
    "from mindspore import Parameter\n",
    "from mindspore.nn.loss.loss import _Loss\n",
    "from mindspore.ops import functional as F\n",
    "from mindspore.ops import operations as P\n",
    "from mindspore.common import dtype as mstype"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 设置运行环境"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "context.set_context(mode=context.GRAPH_MODE, save_graphs=False, device_target='Ascend')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 查看数据内容"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hi.\t嗨。\n",
      "\n",
      "Hi.\t你好。\n",
      "\n",
      "Run.\t你用跑的。\n",
      "\n",
      "Wait!\t等等！\n",
      "\n",
      "Hello!\t你好。\n",
      "\n",
      "I try.\t让我来。\n",
      "\n",
      "I won!\t我赢了。\n",
      "\n",
      "Oh no!\t不会吧。\n",
      "\n",
      "Cheers!\t干杯!\n",
      "\n",
      "He ran.\t他跑了。\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#查看训练数据内容前10行内容\n",
    "with open(\"cmn_zhsim.txt\", 'r', encoding='utf-8') as f:\n",
    "        for i in range(10):\n",
    "            print(f.readline())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 定义数据预处理函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "EOS = \"<eos>\"\n",
    "SOS = \"<sos>\"\n",
    "MAX_SEQ_LEN=10\n",
    "#我们需要将字符转化为ASCII编码\n",
    "#并全部转化为小写字母，并修剪大部分标点符号\n",
    "#除了(a-z, A-Z, \".\", \"?\", \"!\", \",\")这些字符外，全替换成空格\n",
    "def unicodeToAscii(s):\n",
    "    return ''.join(\n",
    "        c for c in unicodedata.normalize('NFD', s)\n",
    "        if unicodedata.category(c) != 'Mn'\n",
    "    )\n",
    "\n",
    "def normalizeString(s):\n",
    "    s = s.lower().strip()\n",
    "    s = unicodeToAscii(s)\n",
    "    s = re.sub(r\"([.!?])\", r\" \\1\", s)\n",
    "    s = re.sub(r\"[^a-zA-Z.!?]+\", r\" \", s)\n",
    "    return s\n",
    "\n",
    "def prepare_data(data_path, vocab_save_path, max_seq_len):\n",
    "    with open(data_path, 'r', encoding='utf-8') as f:\n",
    "        data = f.read()\n",
    "\n",
    "    # 读取文本文件，按行分割，再将每行分割成语句对\n",
    "    data = data.split('\\n')\n",
    "\n",
    "    # 截取前2000行数据进行训练\n",
    "    data = data[:2000]\n",
    "\n",
    "    # 分割每行中的中英文\n",
    "    en_data = [normalizeString(line.split('\\t')[0]) for line in data]\n",
    "\n",
    "    ch_data = [line.split('\\t')[1] for line in data]\n",
    "\n",
    "    # 利用集合，获得中英文词汇表\n",
    "    en_vocab = set(' '.join(en_data).split(' '))\n",
    "    id2en = [EOS] + [SOS] + list(en_vocab)\n",
    "    en2id = {c:i for i,c in enumerate(id2en)}\n",
    "    en_vocab_size = len(id2en)\n",
    "    # np.savetxt(os.path.join(vocab_save_path, 'en_vocab.txt'), np.array(id2en), fmt='%s')\n",
    "\n",
    "    ch_vocab = set(''.join(ch_data))\n",
    "    id2ch = [EOS] + [SOS] + list(ch_vocab)\n",
    "    ch2id = {c:i for i,c in enumerate(id2ch)}\n",
    "    ch_vocab_size = len(id2ch)\n",
    "    # np.savetxt(os.path.join(vocab_save_path, 'ch_vocab.txt'), np.array(id2ch), fmt='%s')\n",
    "\n",
    "    # 将句子用词汇表id表示\n",
    "    en_num_data = np.array([[1] + [int(en2id[en]) for en in line.split(' ')] + [0] for line in en_data],dtype=object)\n",
    "    ch_num_data = np.array([[1] + [int(ch2id[ch]) for ch in line] + [0] for line in ch_data],dtype=object)\n",
    "\n",
    "    #将短句子扩充到统一的长度\n",
    "    for i in range(len(en_num_data)):\n",
    "        num = max_seq_len + 1 - len(en_num_data[i])\n",
    "        if(num >= 0):\n",
    "            en_num_data[i] += [0]*num\n",
    "        else:\n",
    "            en_num_data[i] = en_num_data[i][:max_seq_len] + [0]\n",
    "\n",
    "    for i in range(len(ch_num_data)):\n",
    "        num = max_seq_len + 1 - len(ch_num_data[i])\n",
    "        if(num >= 0):\n",
    "            ch_num_data[i] += [0]*num\n",
    "        else:\n",
    "            ch_num_data[i] = ch_num_data[i][:max_seq_len] + [0]\n",
    "    \n",
    "    \n",
    "    np.savetxt(os.path.join(vocab_save_path, 'en_vocab.txt'), np.array(id2en), fmt='%s')\n",
    "    \n",
    "    np.savetxt(os.path.join(vocab_save_path, 'ch_vocab.txt'), np.array(id2ch), fmt='%s')\n",
    "\n",
    "    return en_num_data, ch_num_data, en_vocab_size, ch_vocab_size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 获得MindRecord文件"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 1040, 623, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[1, 1040, 623, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[1, 230, 623, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[1, 292, 719, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[1, 14, 719, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[1, 469, 34, 623, 0, 0, 0, 0, 0, 0, 0]\n",
      "[1, 469, 847, 719, 0, 0, 0, 0, 0, 0, 0]\n",
      "[1, 931, 240, 719, 0, 0, 0, 0, 0, 0, 0]\n",
      "[1, 817, 719, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[1, 1081, 1116, 623, 0, 0, 0, 0, 0, 0, 0]\n",
      "en_vocab_size:  1154\n",
      "ch_vocab_size:  1116\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(1154, 1116)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#将处理后的数据保存为mindrecord文件，方便后续训练\n",
    "def convert_to_mindrecord(data_path, mindrecord_save_path, max_seq_len):\n",
    "    en_num_data, ch_num_data, en_vocab_size, ch_vocab_size = prepare_data(data_path, mindrecord_save_path, max_seq_len)\n",
    "\n",
    "    # 输出前十行英文句子对应的数据\n",
    "    for i in range(10):\n",
    "        print(en_num_data[i])\n",
    "    \n",
    "    data_list_train = []\n",
    "    for en, ch in zip(en_num_data, ch_num_data):\n",
    "        en = np.array(en).astype(np.int32)\n",
    "        ch = np.array(ch).astype(np.int32)\n",
    "        data_json = {\"encoder_data\": en.reshape(-1),\n",
    "                        \"decoder_data\": ch.reshape(-1)}\n",
    "        data_list_train.append(data_json)\n",
    "    \n",
    "    data_list_eval = random.sample(data_list_train, 20)\n",
    "\n",
    "    data_dir = os.path.join(mindrecord_save_path, \"gru_train.mindrecord\")\n",
    "    writer = FileWriter(data_dir)\n",
    "    schema_json = {\"encoder_data\": {\"type\": \"int32\", \"shape\": [-1]},\n",
    "                    \"decoder_data\": {\"type\": \"int32\", \"shape\": [-1]}}\n",
    "    writer.add_schema(schema_json, \"gru_schema\")\n",
    "    writer.write_raw_data(data_list_train)\n",
    "    writer.commit()\n",
    "\n",
    "    data_dir = os.path.join(mindrecord_save_path, \"gru_eval.mindrecord\")\n",
    "    writer = FileWriter(data_dir)\n",
    "    writer.add_schema(schema_json, \"gru_schema\")\n",
    "    writer.write_raw_data(data_list_eval)\n",
    "    writer.commit()\n",
    "\n",
    "    print(\"en_vocab_size: \", en_vocab_size)\n",
    "    print(\"ch_vocab_size: \", ch_vocab_size)\n",
    "\n",
    "    return en_vocab_size, ch_vocab_size\n",
    "\n",
    "#调用该函数将数据转换成mindrecord格式，让mindspore能够读取\n",
    "if not os.path.exists(\"./preprocess\"):\n",
    "    os.mkdir('./preprocess')\n",
    "convert_to_mindrecord(\"cmn_zhsim.txt\", './preprocess', MAX_SEQ_LEN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 超参数设置"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from easydict import EasyDict as edict\n",
    "\n",
    "# CONFIG\n",
    "cfg = edict({\n",
    "    'en_vocab_size': 1154,\n",
    "    'ch_vocab_size': 1116,\n",
    "    'max_seq_length': 10,\n",
    "    'hidden_size': 1024,\n",
    "    'batch_size': 16,\n",
    "    'eval_batch_size': 1,\n",
    "    'learning_rate': 0.001,\n",
    "    'momentum': 0.9,\n",
    "    'num_epochs': 15,\n",
    "    'save_checkpoint_steps': 125,\n",
    "    'keep_checkpoint_max': 10,\n",
    "    'dataset_path':'./preprocess',\n",
    "    'ckpt_save_path':'./ckpt',\n",
    "    'checkpoint_path':'./ckpt/gru-15_125.ckpt'\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 定义读取MindRecord函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def target_operation(encoder_data, decoder_data):\n",
    "    encoder_data = encoder_data[1:]\n",
    "    target_data = decoder_data[1:]\n",
    "    decoder_data = decoder_data[:-1]\n",
    "    return encoder_data, decoder_data, target_data\n",
    "\n",
    "def eval_operation(encoder_data, decoder_data):\n",
    "    encoder_data = encoder_data[1:]\n",
    "    decoder_data = decoder_data[:-1]\n",
    "    return encoder_data, decoder_data\n",
    "\n",
    "def create_dataset(data_home, batch_size, repeat_num=1, is_training=True, device_num=1, rank=0):\n",
    "    if is_training:\n",
    "        data_dir = os.path.join(data_home, \"gru_train.mindrecord\")\n",
    "    else:\n",
    "        data_dir = os.path.join(data_home, \"gru_eval.mindrecord\")\n",
    "    data_set = ds.MindDataset(data_dir, columns_list=[\"encoder_data\",\"decoder_data\"], \n",
    "                                num_parallel_workers=4,\n",
    "                                num_shards=device_num, shard_id=rank)\n",
    "    if is_training:\n",
    "        operations = target_operation\n",
    "        data_set = data_set.map(operations=operations, \n",
    "                                input_columns=[\"encoder_data\",\"decoder_data\"],\n",
    "                    output_columns=[\"encoder_data\",\"decoder_data\",\"target_data\"],\n",
    "                    column_order=[\"encoder_data\",\"decoder_data\",\"target_data\"])\n",
    "    else:\n",
    "        operations = eval_operation\n",
    "        data_set = data_set.map(operations=operations, \n",
    "                                input_columns=[\"encoder_data\",\"decoder_data\"],\n",
    "                                output_columns=[\"encoder_data\",\"decoder_data\"],\n",
    "                                column_order=[\"encoder_data\",\"decoder_data\"])\n",
    "    data_set = data_set.shuffle(buffer_size=data_set.get_dataset_size())\n",
    "    data_set = data_set.batch(batch_size=batch_size, drop_remainder=True)\n",
    "    data_set = data_set.repeat(count=repeat_num)\n",
    "    return data_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_train = create_dataset(cfg.dataset_path, cfg.batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 定义GRU单元"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gru_default_state(batch_size, input_size, hidden_size, num_layers=1, bidirectional=False):\n",
    "    '''Weight init for gru cell'''\n",
    "    stdv = 1 / math.sqrt(hidden_size)\n",
    "    weight_i = Parameter(Tensor(\n",
    "        np.random.uniform(-stdv, stdv, (input_size, 3*hidden_size)).astype(np.float32)), \n",
    "                            name='weight_i')\n",
    "    weight_h = Parameter(Tensor(\n",
    "        np.random.uniform(-stdv, stdv, (hidden_size, 3*hidden_size)).astype(np.float32)), \n",
    "                            name='weight_h')\n",
    "    bias_i = Parameter(Tensor(\n",
    "        np.random.uniform(-stdv, stdv, (3*hidden_size)).astype(np.float32)), name='bias_i')\n",
    "    bias_h = Parameter(Tensor(\n",
    "        np.random.uniform(-stdv, stdv, (3*hidden_size)).astype(np.float32)), name='bias_h')\n",
    "    return weight_i, weight_h, bias_i, bias_h\n",
    "\n",
    "class GRU(nn.Cell):\n",
    "    def __init__(self, config, is_training=True):\n",
    "        super(GRU, self).__init__()\n",
    "        if is_training:\n",
    "            self.batch_size = config.batch_size\n",
    "        else:\n",
    "            self.batch_size = config.eval_batch_size\n",
    "        self.hidden_size = config.hidden_size\n",
    "        self.weight_i, self.weight_h, self.bias_i, self.bias_h = \\\n",
    "            gru_default_state(self.batch_size, self.hidden_size, self.hidden_size)\n",
    "        self.rnn = P.DynamicGRUV2()\n",
    "        self.cast = P.Cast()\n",
    "\n",
    "    def construct(self, x, hidden):\n",
    "        x = self.cast(x, mstype.float16)\n",
    "        y1, h1, _, _, _, _ = self.rnn(x, self.weight_i, self.weight_h, self.bias_i, self.bias_h, None, hidden)\n",
    "        return y1, h1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 定义Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Cell):\n",
    "    def __init__(self, config, is_training=True):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.vocab_size = config.en_vocab_size\n",
    "        self.hidden_size = config.hidden_size\n",
    "        if is_training:\n",
    "            self.batch_size = config.batch_size\n",
    "        else:\n",
    "            self.batch_size = config.eval_batch_size\n",
    "\n",
    "        self.trans = P.Transpose()\n",
    "        self.perm = (1, 0, 2)\n",
    "        self.embedding = nn.Embedding(self.vocab_size, self.hidden_size)\n",
    "        self.gru = GRU(config, is_training=is_training).to_float(mstype.float16)\n",
    "        self.h = Tensor(np.zeros((self.batch_size, self.hidden_size)).astype(np.float16))\n",
    "\n",
    "    def construct(self, encoder_input):\n",
    "        embeddings = self.embedding(encoder_input)\n",
    "        embeddings = self.trans(embeddings, self.perm)\n",
    "        output, hidden = self.gru(embeddings, self.h)\n",
    "        return output, hidden"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 定义Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Cell):\n",
    "    def __init__(self, config, is_training=True, dropout=0.1):\n",
    "        super(Decoder, self).__init__()\n",
    "\n",
    "        self.vocab_size = config.ch_vocab_size\n",
    "        self.hidden_size = config.hidden_size\n",
    "        self.max_len = config.max_seq_length\n",
    "\n",
    "        self.trans = P.Transpose()\n",
    "        self.perm = (1, 0, 2)\n",
    "        self.embedding = nn.Embedding(self.vocab_size, self.hidden_size)\n",
    "        self.dropout = nn.Dropout(1-dropout)\n",
    "        self.attn = nn.Dense(self.hidden_size, self.max_len)\n",
    "        self.softmax = nn.Softmax(axis=2)\n",
    "        self.bmm = P.BatchMatMul()\n",
    "        self.concat = P.Concat(axis=2)\n",
    "        self.attn_combine = nn.Dense(self.hidden_size * 2, self.hidden_size)\n",
    "\n",
    "        self.gru = GRU(config, is_training=is_training).to_float(mstype.float16)\n",
    "        self.out = nn.Dense(self.hidden_size, self.vocab_size)\n",
    "        self.logsoftmax = nn.LogSoftmax(axis=2)\n",
    "        self.cast = P.Cast()\n",
    "\n",
    "    def construct(self, decoder_input, hidden, encoder_output):\n",
    "        embeddings = self.embedding(decoder_input)\n",
    "        embeddings = self.dropout(embeddings)\n",
    "        # calculate attn\n",
    "        attn_weights = self.softmax(self.attn(embeddings)) # [1,1,10]\n",
    "        encoder_output = self.trans(encoder_output, self.perm)\n",
    "        attn_applied = self.bmm(attn_weights, self.cast(encoder_output,mstype.float32))\n",
    "        output =  self.concat((embeddings, attn_applied))\n",
    "        output = self.attn_combine(output)\n",
    "\n",
    "\n",
    "        embeddings = self.trans(embeddings, self.perm)\n",
    "        output, hidden = self.gru(embeddings, hidden)\n",
    "        output = self.cast(output, mstype.float32)\n",
    "        output = self.out(output)\n",
    "        output = self.logsoftmax(output)\n",
    "\n",
    "        return output, hidden, attn_weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 定义Seq2Seq整体结构"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Seq2Seq(nn.Cell):\n",
    "    def __init__(self, config, is_train=True):\n",
    "        super(Seq2Seq, self).__init__()\n",
    "        self.max_len = config.max_seq_length\n",
    "        self.is_train = is_train\n",
    "\n",
    "        self.encoder = Encoder(config, is_train)\n",
    "        self.decoder = Decoder(config, is_train)\n",
    "        self.expanddims = P.ExpandDims()\n",
    "        self.squeeze = P.Squeeze(axis=0)\n",
    "        self.argmax = P.ArgMaxWithValue(axis=int(2), keep_dims=True)\n",
    "        self.concat = P.Concat(axis=1)\n",
    "        self.concat2 = P.Concat(axis=0)\n",
    "        self.select = P.Select()\n",
    "\n",
    "    def construct(self, src, dst):\n",
    "        encoder_output, hidden = self.encoder(src)\n",
    "        decoder_hidden = self.squeeze(encoder_output[self.max_len-2:self.max_len-1:1, ::, ::])\n",
    "        if self.is_train:\n",
    "            outputs, _ = self.decoder(dst, decoder_hidden, encoder_output)\n",
    "        else:\n",
    "            decoder_input = dst[::,0:1:1]\n",
    "            decoder_outputs = ()\n",
    "            for i in range(0, self.max_len):\n",
    "                decoder_output, decoder_hidden, _ = self.decoder(decoder_input, \n",
    "                                                                    decoder_hidden, encoder_output)\n",
    "                decoder_hidden = self.squeeze(decoder_hidden)\n",
    "                decoder_output, _ = self.argmax(decoder_output)\n",
    "                decoder_output = self.squeeze(decoder_output)\n",
    "                decoder_outputs += (decoder_output,)\n",
    "                decoder_input = decoder_output\n",
    "            outputs = self.concat(decoder_outputs)\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 定义损失函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NLLLoss(_Loss):\n",
    "    '''\n",
    "        NLLLoss function\n",
    "    '''\n",
    "    def __init__(self, reduction='mean'):\n",
    "        super(NLLLoss, self).__init__(reduction)\n",
    "        self.one_hot = P.OneHot()\n",
    "        self.reduce_sum = P.ReduceSum()\n",
    "\n",
    "    def construct(self, logits, label):\n",
    "        label_one_hot = self.one_hot(label, F.shape(logits)[-1], F.scalar_to_array(1.0), \n",
    "                                        F.scalar_to_array(0.0))\n",
    "        #print('NLLLoss label_one_hot:',label_one_hot, label_one_hot.shape)\n",
    "        #print('NLLLoss logits:',logits, logits.shape)\n",
    "        #print('xxx:', logits * label_one_hot)\n",
    "        loss = self.reduce_sum(-1.0 * logits * label_one_hot, (1,))\n",
    "        return self.get_loss(loss)\n",
    "    \n",
    "class WithLossCell(nn.Cell):\n",
    "    def __init__(self, backbone, config):\n",
    "        super(WithLossCell, self).__init__(auto_prefix=False)\n",
    "        self._backbone = backbone\n",
    "        self.batch_size = config.batch_size\n",
    "        self.onehot = nn.OneHot(depth=config.ch_vocab_size)\n",
    "        self._loss_fn = NLLLoss()\n",
    "        self.max_len = config.max_seq_length\n",
    "        self.squeeze = P.Squeeze()\n",
    "        self.cast = P.Cast()\n",
    "        self.argmax = P.ArgMaxWithValue(axis=1, keep_dims=True)\n",
    "        self.print = P.Print()\n",
    "\n",
    "    def construct(self, src, dst, label):\n",
    "        out = self._backbone(src, dst)\n",
    "        loss_total = 0\n",
    "        for i in range(self.batch_size):\n",
    "            loss = self._loss_fn(self.squeeze(out[::,i:i+1:1,::]), \n",
    "                                    self.squeeze(label[i:i+1:1, ::]))\n",
    "            loss_total += loss\n",
    "        loss = loss_total / self.batch_size\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 定义训练网络和优化器"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[WARNING] ME(20944:28404,MainProcess):2022-11-27-15:48:19.814.909 [mindspore\\nn\\loss\\loss.py:113] '_Loss' is deprecated from version 1.3 and will be removed in a future version, use 'LossBase' instead.\n"
     ]
    }
   ],
   "source": [
    "network = Seq2Seq(cfg)\n",
    "network = WithLossCell(network, cfg)\n",
    "optimizer = nn.Adam(network.trainable_params(), learning_rate=cfg.learning_rate, beta1=0.9, beta2=0.98)\n",
    "model = Model(network, optimizer=optimizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 定义回调函数、构建模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_cb = LossMonitor()\n",
    "config_ck = CheckpointConfig(save_checkpoint_steps=cfg.save_checkpoint_steps, keep_checkpoint_max=cfg.keep_checkpoint_max)\n",
    "ckpoint_cb = ModelCheckpoint(prefix=\"gru\", directory=cfg.ckpt_save_path, config=config_ck)\n",
    "time_cb = TimeMonitor(data_size=ds_train.get_dataset_size())\n",
    "callbacks = [time_cb, ckpoint_cb, loss_cb]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 模型训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[WARNING] ME(20944:28404,MainProcess):2022-11-27-16:10:43.611.740 [mindspore\\train\\model.py:500] The CPU cannot support dataset sink mode currently.So the training process will be performed with dataset not sink.\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "mindspore\\ccsrc\\backend\\kernel_compiler\\cpu\\cpu_kernel_factory.cc:169 GetSupportedKernelAttrList] Not registered CPU kernel: op[DynamicGRUV2]!\n\n# In file c:\\Anaconda\\envs\\MindSpore1.5\\lib\\site-packages\\mindspore\\ops\\composite\\multitype_ops\\_compile_utils.py(405)\n    return P.StridedSlice(0, 0, 0, 0, shrink_axis_mask)(data, begin_strides, end_strides, step_strides)\n           ^\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_20944\\1406904949.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcfg\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnum_epochs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mds_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdataset_sink_mode\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mc:\\Anaconda\\envs\\MindSpore1.5\\lib\\site-packages\\mindspore\\train\\model.py\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(self, epoch, train_dataset, callbacks, dataset_sink_mode, sink_size)\u001b[0m\n\u001b[0;32m    724\u001b[0m                     \u001b[0mcallbacks\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    725\u001b[0m                     \u001b[0mdataset_sink_mode\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdataset_sink_mode\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 726\u001b[1;33m                     sink_size=sink_size)\n\u001b[0m\u001b[0;32m    727\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    728\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mbuild\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_dataset\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalid_dataset\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msink_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Anaconda\\envs\\MindSpore1.5\\lib\\site-packages\\mindspore\\train\\model.py\u001b[0m in \u001b[0;36m_train\u001b[1;34m(self, epoch, train_dataset, callbacks, dataset_sink_mode, sink_size)\u001b[0m\n\u001b[0;32m    500\u001b[0m                 logger.warning(\"The CPU cannot support dataset sink mode currently.\"\n\u001b[0;32m    501\u001b[0m                                \"So the training process will be performed with dataset not sink.\")\n\u001b[1;32m--> 502\u001b[1;33m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_train_process\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_dataset\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlist_callback\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcb_params\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    503\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    504\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_train_dataset_sink_process\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_dataset\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlist_callback\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcb_params\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msink_size\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Anaconda\\envs\\MindSpore1.5\\lib\\site-packages\\mindspore\\train\\model.py\u001b[0m in \u001b[0;36m_train_process\u001b[1;34m(self, epoch, train_dataset, list_callback, cb_params)\u001b[0m\n\u001b[0;32m    624\u001b[0m                 \u001b[0mcb_params\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain_dataset_element\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnext_element\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    625\u001b[0m                 \u001b[0mlist_callback\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep_begin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_context\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 626\u001b[1;33m                 \u001b[0moutputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_train_network\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mnext_element\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    627\u001b[0m                 \u001b[0mcb_params\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnet_outputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0moutputs\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    628\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_loss_scale_manager\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_loss_scale_manager\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_drop_overflow_update\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Anaconda\\envs\\MindSpore1.5\\lib\\site-packages\\mindspore\\nn\\cell.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *inputs, **kwargs)\u001b[0m\n\u001b[0;32m    402\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0menable_hook\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    403\u001b[0m                 \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"The graph mode does not support hook function.\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 404\u001b[1;33m             \u001b[0mout\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcompile_and_run\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    405\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mout\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    406\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Anaconda\\envs\\MindSpore1.5\\lib\\site-packages\\mindspore\\nn\\cell.py\u001b[0m in \u001b[0;36mcompile_and_run\u001b[1;34m(self, *inputs)\u001b[0m\n\u001b[0;32m    680\u001b[0m         \"\"\"\n\u001b[0;32m    681\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_auto_parallel_compile_and_run\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 682\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcompile\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    683\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    684\u001b[0m         \u001b[0mnew_inputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Anaconda\\envs\\MindSpore1.5\\lib\\site-packages\\mindspore\\nn\\cell.py\u001b[0m in \u001b[0;36mcompile\u001b[1;34m(self, *inputs)\u001b[0m\n\u001b[0;32m    667\u001b[0m             \u001b[0minputs\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mtuple\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mInputs\u001b[0m \u001b[0mof\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mCell\u001b[0m \u001b[0mobject\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    668\u001b[0m         \"\"\"\n\u001b[1;32m--> 669\u001b[1;33m         \u001b[0m_cell_graph_executor\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcompile\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mphase\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mphase\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mauto_parallel_mode\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_auto_parallel_mode\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    670\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    671\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mcompile_and_run\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Anaconda\\envs\\MindSpore1.5\\lib\\site-packages\\mindspore\\common\\api.py\u001b[0m in \u001b[0;36mcompile\u001b[1;34m(self, obj, phase, do_convert, auto_parallel_mode, *args)\u001b[0m\n\u001b[0;32m    546\u001b[0m         \u001b[0menable_ge\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcontext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_context\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"enable_ge\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    547\u001b[0m         \u001b[0muse_vm\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0menable_ge\u001b[0m \u001b[1;32mor\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0menable_debug_runtime\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mcontext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_context\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"mode\"\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mcontext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mPYNATIVE_MODE\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 548\u001b[1;33m         \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_graph_executor\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcompile\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs_list\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mphase\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0muse_vm\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mqueue_name\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    549\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcompile_cache\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mphase\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mphase\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    550\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: mindspore\\ccsrc\\backend\\kernel_compiler\\cpu\\cpu_kernel_factory.cc:169 GetSupportedKernelAttrList] Not registered CPU kernel: op[DynamicGRUV2]!\n\n# In file c:\\Anaconda\\envs\\MindSpore1.5\\lib\\site-packages\\mindspore\\ops\\composite\\multitype_ops\\_compile_utils.py(405)\n    return P.StridedSlice(0, 0, 0, 0, shrink_axis_mask)(data, begin_strides, end_strides, step_strides)\n           ^\n"
     ]
    }
   ],
   "source": [
    "model.train(cfg.num_epochs, ds_train, callbacks=callbacks, dataset_sink_mode=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 定义推理网络"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class InferCell(nn.Cell):\n",
    "    def __init__(self, network, config):\n",
    "        super(InferCell, self).__init__(auto_prefix=False)\n",
    "        self.expanddims = P.ExpandDims()\n",
    "        self.network = network\n",
    "\n",
    "    def construct(self, src, dst):\n",
    "        out = self.network(src, dst)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "network = Seq2Seq(cfg,is_train=False)\n",
    "network = InferCell(network, cfg)\n",
    "network.set_train(False)\n",
    "parameter_dict = load_checkpoint(cfg.checkpoint_path)\n",
    "load_param_into_net(network, parameter_dict)\n",
    "model = Model(network)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 定义翻译测试函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#打开词汇表\n",
    "with open(os.path.join(cfg.dataset_path,\"en_vocab.txt\"), 'r', encoding='utf-8') as f:\n",
    "    data = f.read()\n",
    "en_vocab = list(data.split('\\n'))\n",
    "\n",
    "with open(os.path.join(cfg.dataset_path,\"ch_vocab.txt\"), 'r', encoding='utf-8') as f:\n",
    "    data = f.read()\n",
    "ch_vocab = list(data.split('\\n'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def translate(str_en):\n",
    "    max_seq_len = 10\n",
    "    str_vocab = normalizeString(str_en).split(' ')\n",
    "    print(\"English\",str(str_vocab))\n",
    "    str_id = [1]\n",
    "    for i in str_vocab:\n",
    "        str_id += [en_vocab.index(i)]\n",
    "\n",
    "    num = max_seq_len + 1 - len(str_id)\n",
    "    if(num >= 0):\n",
    "        str_id += [0]*num\n",
    "    else:\n",
    "        str_id = str_id[:max_seq_len] + [0]\n",
    "    str_id = Tensor(np.array([str_id[1:]]).astype(np.int32))\n",
    "\n",
    "    out_id = [1]+[0]*10\n",
    "    out_id = Tensor(np.array([out_id[:-1]]).astype(np.int32))\n",
    "    \n",
    "    output = network(str_id, out_id)\n",
    "    out= ''\n",
    "    for x in output[0].asnumpy():\n",
    "        if x == 0:\n",
    "            break\n",
    "        out += ch_vocab[x]\n",
    "    print(\"中文\",out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 离线加载预测"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "translate('I love Tom ')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.15 ('MindSpore1.5')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.15"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "b481270e4f32a5330818c7d06dca840dd59c8779142471ce787048eca7ccd567"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
